ðŸ’¡ Agent Design & Approach (Report)

This agent is designed as a cyclical, self-correcting graph rather than a simple linear pipeline. This innovative approach enhances answer quality by allowing the agent to learn from its own mistakes in real-time. The workflow is defined by the four required nodes: `plan`, `retrieve`, `answer`, and `reflect`. The `plan` node first acts as a query-rewriter to optimize the user's question for vector search. The `retrieve` node fetches context from a **ChromaDB vector store**, which is populated using **Hugging Face embeddings**. The `answer` node then uses the **Google Gemini LLM** to generate a response based *only* on this context.

The main innovation lies in the `reflect` node and the graph's conditional logic. This node acts as an "LLM-as-a-Judge," using the **Gemini model** again to critically evaluate the generated answer for relevance and faithfulness. If the answer is judged "Good," the process ends. However, if it's judged "Revise," the graph routes back to the `plan` node. This time, the `plan` node receives both the original question and the critique from the reflection. This "Plan-Do-Check-Act" cycle allows the agent to formulate a new, more informed query, retry the retrieval, and generate a superior answer. The main challenge was prompting the `reflect` node to be a strict but fair judge, ensuring it could differentiate a mediocre answer from a good one, which is key to triggering the self-correction loop effectively.


Gemini can make mistakes, so double-check it